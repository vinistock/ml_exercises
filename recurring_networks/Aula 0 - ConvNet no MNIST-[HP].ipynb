{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atenção: Rode esta linha apenas se estiver usando o Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O código da célula abaixo contém funções para efetuar a carga dos dados, treinamento teste dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=datasets.MNIST(\n",
    "            root='../data', \n",
    "            train=True, \n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        ),\n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=datasets.MNIST(\n",
    "            root='../data', \n",
    "            train=False, \n",
    "            download=True,\n",
    "            transform=transform,\n",
    "        ),\n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_epoch(\n",
    "        model, \n",
    "        device, \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        epoch, \n",
    "        log_interval\n",
    "    ):\n",
    "    model.train()\n",
    "    history = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(\n",
    "        model, \n",
    "        device, \n",
    "        criterion, \n",
    "        test_loader\n",
    "    ):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        device,\n",
    "        lr,\n",
    "        nb_epochs=3,\n",
    "        log_interval=100,\n",
    "    ):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    for epoch in range(1, nb_epochs + 1):\n",
    "        print('\\n* * * Training * * *')\n",
    "        train_epoch(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            train_loader=train_loader, \n",
    "            optimizer=optimizer, \n",
    "            criterion=criterion, \n",
    "            epoch=epoch, \n",
    "            log_interval=log_interval\n",
    "        )\n",
    "        print('\\n* * * Evaluating * * *')\n",
    "        acc = test(model, device, criterion, test_loader)        \n",
    "    \n",
    "    return acc\n",
    "\n",
    "def check_input(model, device):\n",
    "    dummy_data = torch.zeros(5, 1, 28, 28).to(device)\n",
    "    dummy_pred = model(dummy_data)        \n",
    "    assert dummy_pred.shape == (5, 10), '\\nOutput expected: (batch_size, 10) \\nOutput found   : {}'.format(dummy_pred.shape)\n",
    "    print('Passed')\n",
    "    return dummy_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parâmetros que você pode definir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "device_name = 'cpu'\n",
    "nb_epochs = 3\n",
    "log_interval = 500\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferência dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_loaders(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  torch.Size([60000, 28, 28]) torch.Size([60000])\n",
      "Test size :  torch.Size([10000, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Train size: ', \n",
    "    train_loader.dataset.train_data.shape, \n",
    "    train_loader.dataset.train_labels.shape\n",
    ")\n",
    "print(\n",
    "    'Test size : ', \n",
    "    test_loader.dataset.test_data.shape, \n",
    "    test_loader.dataset.test_labels.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 5)\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(train_loader.dataset.train_data[i], cmap='gray')\n",
    "    ax.set_title(train_loader.dataset.train_labels[i].item())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance Example:  torch.Size([16, 1, 28, 28]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "instance = next(iter(train_loader))\n",
    "print('Instance Example: ', instance[0].shape, instance[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seu trabalho começa aqui:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implemente aqui sua primeira rede convolucional  \n",
    "\n",
    "Sua ConvNet deve ser capaz de classificar as imagens do MNIST. Lembre-se que as imagens do MNIST tem apenas 1 canal, isto é, elas são em escala de cinza (e não RBG!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(in_features=3136, out_features=10)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], 64*7*7) # Ou x = x.view(x.shape[0], -1)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Verifique se a saída do seu modelo está correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "dummy_pred = check_input(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Treine seu modelo por uma ou mais épocas. \n",
    "\n",
    "Você deve conseguir ~99% de acurácia na terceira época. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301813\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.008383\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.137078\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.081720\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.028000\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.021742\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.018145\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.000749\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0029, Accuracy: 9852/10000 (98.52%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.007783\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.233236\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.005254\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.001548\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.016889\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.007922\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.077646\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.001384\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0022, Accuracy: 9887/10000 (98.87%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.004755\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.010951\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.005922\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.002941\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.008249\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.000173\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.001478\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0020, Accuracy: 9896/10000 (98.96%)\n",
      "\n",
      "Final acc: 98.96%\n"
     ]
    }
   ],
   "source": [
    "acc = train(model, train_loader, test_loader, device, lr, nb_epochs, log_interval)\n",
    "print('Final acc: {:.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Atualize sua rede convolucional para usar o container nn.Sequential()\n",
    "\n",
    "A arquitetura da rede pode ser exatamente igual à rede anterior, porém, agora use o nn.Sequential para criar as camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class ConvNetSeq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetSeq, self).__init__()\n",
    "\n",
    "        self.seq = nn.Sequential(*[\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features = 3136, out_features = 10)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        out = self.seq(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "model = ConvNetSeq().to(device)\n",
    "dummy_pred = check_input(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.344187\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.011054\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.001904\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.001668\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.026790\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.033658\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.078309\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.007458\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0027, Accuracy: 9863/10000 (98.63%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.144584\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.025954\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.001706\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.001082\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.004851\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.000145\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.340389\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.041642\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0020, Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001025\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.001816\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.003558\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.006451\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.005868\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.001585\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.024680\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.003588\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0024, Accuracy: 9884/10000 (98.84%)\n",
      "\n",
      "Final acc: 98.84%\n"
     ]
    }
   ],
   "source": [
    "acc = train(model, train_loader, test_loader, device, lr, nb_epochs, log_interval)\n",
    "print('Final acc: {:.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crie uma nova rede substituindo as camadas de convolução da sua rede anterior por blocos Inception.  \n",
    "\n",
    "Detalhes:\n",
    "\n",
    "1. Crie um novo módulo (classe que herda do nn.Module) chamado de InceptionModule. \n",
    "2. Nesse módulo você deverá criar camadas convolucionais com filtros 1x1, 3x3 e 5x5 paralelamente. No final, concatene o resultado, e aplique mais uma convolução 1x1 para reduzir a dimensionalidade ao tamanho original. \n",
    "2. Atualize sua rede convolucional substituindo as camadas de convolução pelo seu bloco Inception. \n",
    "3. Treine o modelo e reporte a acurácia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1, padding = 0)\n",
    "        self.conv_2 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
    "        self.conv_3 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 5, padding = 2)\n",
    "        self.conv_4 = nn.Conv2d(in_channels = 3*out_channels, out_channels = out_channels, kernel_size = 1, padding = 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.conv_1(x)\n",
    "        b = self.conv_2(x)\n",
    "        c = self.conv_3(x)\n",
    "\n",
    "        concat = torch.cat((a, b, c), 1)\n",
    "        out = self.conv_4(concat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionNet(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(InceptionNet, self).__init__()  \n",
    "        \n",
    "        self.seq = nn.Sequential(*[\n",
    "            InceptionModule(1, 32),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            InceptionModule(32, 64),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features = 3136, out_features = 10)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.seq(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "model = InceptionNet().to(device)\n",
    "dummy_pred = check_input(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309917\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.064380\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.830235\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.337451\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.009634\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.006171\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.157556\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.202445\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0028, Accuracy: 9865/10000 (98.65%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.002487\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.019056\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.000087\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.002717\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.038038\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.001036\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.004671\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.001712\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0028, Accuracy: 9856/10000 (98.56%)\n",
      "\n",
      "\n",
      "* * * Training * * *\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001361\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.017183\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.000493\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.001908\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.000480\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.000268\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.014063\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.014333\n",
      "\n",
      "* * * Evaluating * * *\n",
      "Test set: Average loss: 0.0027, Accuracy: 9875/10000 (98.75%)\n",
      "\n",
      "Final acc: 98.75%\n"
     ]
    }
   ],
   "source": [
    "acc = train(model, train_loader, test_loader, device, lr, nb_epochs, log_interval)\n",
    "print('Final acc: {:.2f}%'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
